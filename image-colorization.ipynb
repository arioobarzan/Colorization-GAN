{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-06-02T13:14:28.360217Z",
     "iopub.status.busy": "2023-06-02T13:14:28.359518Z",
     "iopub.status.idle": "2023-06-02T13:14:32.030532Z",
     "shell.execute_reply": "2023-06-02T13:14:32.029611Z",
     "shell.execute_reply.started": "2023-06-02T13:14:28.360183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "None\n",
      "1.13.1+cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import rgb2lab, lab2rgb\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "print(torch.backends.cudnn.enabled)\n",
    "print(torch.version.cuda)\n",
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: fastai in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (1.0.64.dev0)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from fastai) (1.21.6)\n",
      "Requirement already satisfied: requests in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from fastai) (2.31.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from fastai) (6.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from fastai) (3.5.3)\n",
      "Requirement already satisfied: torchvision in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from fastai) (0.14.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\platinum\\appdata\\roaming\\python\\python37\\site-packages (from fastai) (4.12.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from fastai) (9.5.0)\n",
      "Requirement already satisfied: bottleneck in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from fastai) (1.3.7)\n",
      "Requirement already satisfied: numexpr in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from fastai) (2.8.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from fastai) (1.3.5)\n",
      "Requirement already satisfied: spacy>=2.0.18 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from fastai) (3.5.3)\n",
      "Requirement already satisfied: fastprogress>=0.2.1 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from fastai) (1.0.3)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from fastai) (1.13.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from fastai) (23.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from fastai) (1.7.3)\n",
      "Requirement already satisfied: nvidia-ml-py3 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from fastai) (7.352.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from spacy>=2.0.18->fastai) (2.0.7)\n",
      "Requirement already satisfied: setuptools in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from spacy>=2.0.18->fastai) (68.0.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from spacy>=2.0.18->fastai) (1.10.9)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from spacy>=2.0.18->fastai) (0.10.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from spacy>=2.0.18->fastai) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from spacy>=2.0.18->fastai) (1.0.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from spacy>=2.0.18->fastai) (3.0.12)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from spacy>=2.0.18->fastai) (2.4.6)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from spacy>=2.0.18->fastai) (8.1.10)\n",
      "Requirement already satisfied: typing-extensions<4.5.0,>=3.7.4.1 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from spacy>=2.0.18->fastai) (4.4.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from spacy>=2.0.18->fastai) (1.1.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from spacy>=2.0.18->fastai) (3.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from spacy>=2.0.18->fastai) (2.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from spacy>=2.0.18->fastai) (6.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from spacy>=2.0.18->fastai) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from spacy>=2.0.18->fastai) (4.65.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from spacy>=2.0.18->fastai) (3.3.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from spacy>=2.0.18->fastai) (0.7.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from requests->fastai) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from requests->fastai) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from requests->fastai) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from requests->fastai) (2.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\platinum\\appdata\\roaming\\python\\python37\\site-packages (from beautifulsoup4->fastai) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from matplotlib->fastai) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from matplotlib->fastai) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from matplotlib->fastai) (3.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from matplotlib->fastai) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from matplotlib->fastai) (4.38.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from pandas->fastai) (2023.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from catalogue<2.1.0,>=2.0.6->spacy>=2.0.18->fastai) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->fastai) (1.16.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.0.18->fastai) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.0.18->fastai) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy>=2.0.18->fastai) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy>=2.0.18->fastai) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from jinja2->spacy>=2.0.18->fastai) (2.1.3)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\platinum\\anaconda3\\envs\\deoldify\\lib\\site-packages (from click<9.0.0,>=7.1.1->typer<0.8.0,>=0.3.0->spacy>=2.0.18->fastai) (6.7.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install fastai\n",
    "\n",
    "from fastai.data.external import untar_data, URLs\n",
    "coco_path = untar_data(URLs.COCO_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T13:14:32.033098Z",
     "iopub.status.busy": "2023-06-02T13:14:32.032493Z",
     "iopub.status.idle": "2023-06-02T13:18:32.365175Z",
     "shell.execute_reply": "2023-06-02T13:18:32.364098Z",
     "shell.execute_reply.started": "2023-06-02T13:14:32.033065Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "coco_path = str(coco_path) + \"/train_sample\"\n",
    "use_colab = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T13:18:32.367851Z",
     "iopub.status.busy": "2023-06-02T13:18:32.367228Z",
     "iopub.status.idle": "2023-06-02T13:18:32.502892Z",
     "shell.execute_reply": "2023-06-02T13:18:32.501714Z",
     "shell.execute_reply.started": "2023-06-02T13:18:32.367816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 2000\n"
     ]
    }
   ],
   "source": [
    "paths = glob.glob(coco_path + \"/*.jpg\") # Grabbing all the image file names\n",
    "np.random.seed(123)\n",
    "paths_subset = np.random.choice(paths, 10000, replace=False) # choosing 10000 images randomly\n",
    "rand_idxs = np.random.permutation(10000)\n",
    "train_idxs = rand_idxs[:8000] # choosing the first 8000 as training set\n",
    "val_idxs = rand_idxs[8000:] # choosing last 2000 as validation set\n",
    "train_paths = paths_subset[train_idxs]\n",
    "val_paths = paths_subset[val_idxs]\n",
    "print(len(train_paths), len(val_paths))\n",
    "\n",
    "np.savetxt(\"train_paths.csv\", train_paths, fmt = \"%s\")\n",
    "np.savetxt(\"val_paths.csv\", val_paths, fmt = \"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T13:18:32.507526Z",
     "iopub.status.busy": "2023-06-02T13:18:32.507168Z",
     "iopub.status.idle": "2023-06-02T13:18:41.531454Z",
     "shell.execute_reply": "2023-06-02T13:18:41.530345Z",
     "shell.execute_reply.started": "2023-06-02T13:18:32.507493Z"
    }
   },
   "outputs": [],
   "source": [
    "#Creating Data Loaders\n",
    "SIZE = 256\n",
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, paths, split='train'):\n",
    "        if split == 'train':\n",
    "            self.transforms = transforms.Compose([\n",
    "                transforms.Resize((SIZE, SIZE),  Image.BICUBIC),\n",
    "                transforms.RandomHorizontalFlip(), # A little data augmentation!\n",
    "            ])\n",
    "        elif split == 'val':\n",
    "            self.transforms = transforms.Resize((SIZE, SIZE),  Image.BICUBIC)\n",
    "        \n",
    "        self.split = split\n",
    "        self.size = SIZE\n",
    "        self.paths = paths\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
    "        img = self.transforms(img)\n",
    "        img = np.array(img)\n",
    "        img_lab = rgb2lab(img).astype(\"float32\") # Converting RGB to L*a*b\n",
    "        img_lab = transforms.ToTensor()(img_lab)\n",
    "        L = img_lab[[0], ...] / 50. - 1. # Between -1 and 1\n",
    "        ab = img_lab[[1, 2], ...] / 110. # Between -1 and 1\n",
    "        \n",
    "        return {'L': L, 'ab': ab}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "def make_dataloaders(batch_size=8, n_workers=4, pin_memory=True, **kwargs): # A handy function to make our dataloaders\n",
    "    dataset = ColorizationDataset(**kwargs)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=n_workers,\n",
    "                            pin_memory=pin_memory)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dl = make_dataloaders(paths=train_paths, split='train')\n",
    "val_dl = make_dataloaders(paths=val_paths, split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9440\\2661456618.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mLs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabs_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'L'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ab'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabs_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deoldify\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deoldify\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deoldify\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1032\u001b[0m             \u001b[1;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m             \u001b[1;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1034\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1035\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deoldify\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deoldify\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deoldify\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deoldify\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deoldify\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "\n",
    "data = next(iter(train_dl))\n",
    "Ls, abs_ = data['L'], data['ab']\n",
    "print(Ls.shape, abs_.shape)\n",
    "print(len(train_dl), len(val_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation as Proposed by Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T13:18:43.170517Z",
     "iopub.status.busy": "2023-06-02T13:18:43.170217Z",
     "iopub.status.idle": "2023-06-02T13:18:43.190402Z",
     "shell.execute_reply": "2023-06-02T13:18:43.189364Z",
     "shell.execute_reply.started": "2023-06-02T13:18:43.170490Z"
    }
   },
   "outputs": [],
   "source": [
    "#Generator as Proposed by the Paper\n",
    "class UnetBlock(nn.Module):\n",
    "    def __init__(self, nf, ni, submodule=None, input_c=None, dropout=False,\n",
    "                 innermost=False, outermost=False):\n",
    "        super().__init__()\n",
    "        self.outermost = outermost\n",
    "        if input_c is None: input_c = nf\n",
    "        downconv = nn.Conv2d(input_c, ni, kernel_size=4,\n",
    "                             stride=2, padding=1, bias=False)\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        downnorm = nn.BatchNorm2d(ni)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        upnorm = nn.BatchNorm2d(nf)\n",
    "        \n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4,\n",
    "                                        stride=2, padding=1)\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(ni, nf, kernel_size=4,\n",
    "                                        stride=2, padding=1, bias=False)\n",
    "            down = [downrelu, downconv]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            model = down + up\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4,\n",
    "                                        stride=2, padding=1, bias=False)\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            if dropout: up += [nn.Dropout(0.5)]\n",
    "            model = down + [submodule] + up\n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            return torch.cat([x, self.model(x)], 1)\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, input_c=1, output_c=2, n_down=8, num_filters=64):\n",
    "        super().__init__()\n",
    "        unet_block = UnetBlock(num_filters * 8, num_filters * 8, innermost=True)\n",
    "        for _ in range(n_down - 5):\n",
    "            unet_block = UnetBlock(num_filters * 8, num_filters * 8, submodule=unet_block, dropout=True)\n",
    "        out_filters = num_filters * 8\n",
    "        for _ in range(3):\n",
    "            unet_block = UnetBlock(out_filters // 2, out_filters, submodule=unet_block)\n",
    "            out_filters //= 2\n",
    "        self.model = UnetBlock(output_c, out_filters, input_c=input_c, submodule=unet_block, outermost=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T13:18:43.193516Z",
     "iopub.status.busy": "2023-06-02T13:18:43.192567Z",
     "iopub.status.idle": "2023-06-02T13:18:43.206025Z",
     "shell.execute_reply": "2023-06-02T13:18:43.205147Z",
     "shell.execute_reply.started": "2023-06-02T13:18:43.193481Z"
    }
   },
   "outputs": [],
   "source": [
    "#Discriminator\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, input_c, num_filters=64, n_down=3):\n",
    "        super().__init__()\n",
    "        model = [self.get_layers(input_c, num_filters, norm=False)]\n",
    "        model += [self.get_layers(num_filters * 2 ** i, num_filters * 2 ** (i + 1), s=1 if i == (n_down-1) else 2) \n",
    "                          for i in range(n_down)] # the 'if' statement is taking care of not using\n",
    "                                                  # stride of 2 for the last block in this loop\n",
    "        model += [self.get_layers(num_filters * 2 ** n_down, 1, s=1, norm=False, act=False)] # Make sure to not use normalization or\n",
    "                                                                                             # activation for the last layer of the model\n",
    "        self.model = nn.Sequential(*model)                                                   \n",
    "        \n",
    "    def get_layers(self, ni, nf, k=4, s=2, p=1, norm=True, act=True): # when needing to make some repeatitive blocks of layers,\n",
    "        layers = [nn.Conv2d(ni, nf, k, s, p, bias=not norm)]          # it's always helpful to make a separate method for that purpose\n",
    "        if norm: layers += [nn.BatchNorm2d(nf)]\n",
    "        if act: layers += [nn.LeakyReLU(0.2, True)]\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T13:18:43.207848Z",
     "iopub.status.busy": "2023-06-02T13:18:43.207434Z",
     "iopub.status.idle": "2023-06-02T13:18:43.217922Z",
     "shell.execute_reply": "2023-06-02T13:18:43.217189Z",
     "shell.execute_reply.started": "2023-06-02T13:18:43.207811Z"
    }
   },
   "outputs": [],
   "source": [
    "#GAN Loss Function\n",
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, gan_mode='vanilla', real_label=1.0, fake_label=0.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(fake_label))\n",
    "        if gan_mode == 'vanilla':\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        elif gan_mode == 'lsgan':\n",
    "            self.loss = nn.MSELoss()\n",
    "    \n",
    "    def get_labels(self, preds, target_is_real):\n",
    "        if target_is_real:\n",
    "            labels = self.real_label\n",
    "        else:\n",
    "            labels = self.fake_label\n",
    "        return labels.expand_as(preds)\n",
    "    \n",
    "    def __call__(self, preds, target_is_real):\n",
    "        labels = self.get_labels(preds, target_is_real)\n",
    "        loss = self.loss(preds, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T13:18:43.219999Z",
     "iopub.status.busy": "2023-06-02T13:18:43.219127Z",
     "iopub.status.idle": "2023-06-02T13:18:43.234469Z",
     "shell.execute_reply": "2023-06-02T13:18:43.233725Z",
     "shell.execute_reply.started": "2023-06-02T13:18:43.219968Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_weights(net, init='norm', gain=0.02):\n",
    "    \n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and 'Conv' in classname:\n",
    "            if init == 'norm':\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=gain)\n",
    "            elif init == 'xavier':\n",
    "                nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
    "            elif init == 'kaiming':\n",
    "                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            \n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "        elif 'BatchNorm2d' in classname:\n",
    "            nn.init.normal_(m.weight.data, 1., gain)\n",
    "            nn.init.constant_(m.bias.data, 0.)\n",
    "            \n",
    "    net.apply(init_func)\n",
    "    print(f\"model initialized with {init} initialization\")\n",
    "    return net\n",
    "\n",
    "def init_model(model, device):\n",
    "    model = model.to(device)\n",
    "    model = init_weights(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T13:18:43.238689Z",
     "iopub.status.busy": "2023-06-02T13:18:43.238088Z",
     "iopub.status.idle": "2023-06-02T13:18:43.255033Z",
     "shell.execute_reply": "2023-06-02T13:18:43.254125Z",
     "shell.execute_reply.started": "2023-06-02T13:18:43.238641Z"
    }
   },
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.count, self.avg, self.sum = [0.] * 3\n",
    "    \n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += count * val\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def create_loss_meters():\n",
    "    loss_D_fake = AverageMeter()\n",
    "    loss_D_real = AverageMeter()\n",
    "    loss_D = AverageMeter()\n",
    "    loss_G_GAN = AverageMeter()\n",
    "    loss_G_L1 = AverageMeter()\n",
    "    loss_G = AverageMeter()\n",
    "    \n",
    "    return {'loss_D_fake': loss_D_fake,\n",
    "            'loss_D_real': loss_D_real,\n",
    "            'loss_D': loss_D,\n",
    "            'loss_G_GAN': loss_G_GAN,\n",
    "            'loss_G_L1': loss_G_L1,\n",
    "            'loss_G': loss_G}\n",
    "\n",
    "def update_losses(model, loss_meter_dict, count):\n",
    "    for loss_name, loss_meter in loss_meter_dict.items():\n",
    "        loss = getattr(model, loss_name)\n",
    "        loss_meter.update(loss.item(), count=count)\n",
    "\n",
    "def lab_to_rgb(L, ab):\n",
    "    \"\"\"\n",
    "    Takes a batch of images\n",
    "    \"\"\"\n",
    "    \n",
    "    L = (L + 1.) * 50.\n",
    "    ab = ab * 110.\n",
    "    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n",
    "    rgb_imgs = []\n",
    "    for img in Lab:\n",
    "        img_rgb = lab2rgb(img)\n",
    "        rgb_imgs.append(img_rgb)\n",
    "    return np.stack(rgb_imgs, axis=0)\n",
    "    \n",
    "def visualize(model, data, save=True):\n",
    "    model.net_G.eval()\n",
    "    with torch.no_grad():\n",
    "        model.setup_input(data)\n",
    "        model.forward()\n",
    "    model.net_G.train()\n",
    "    fake_color = model.fake_color.detach()\n",
    "    real_color = model.ab\n",
    "    L = model.L\n",
    "    fake_imgs = lab_to_rgb(L, fake_color)\n",
    "    real_imgs = lab_to_rgb(L, real_color)\n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "    for i in range(5):\n",
    "        ax = plt.subplot(3, 5, i + 1)\n",
    "        ax.imshow(L[i][0].cpu(), cmap='gray')\n",
    "        ax.axis(\"off\")\n",
    "        ax = plt.subplot(3, 5, i + 1 + 5)\n",
    "        ax.imshow(fake_imgs[i])\n",
    "        ax.axis(\"off\")\n",
    "        ax = plt.subplot(3, 5, i + 1 + 10)\n",
    "        ax.imshow(real_imgs[i])\n",
    "        ax.axis(\"off\")\n",
    "    plt.show()\n",
    "    if save:\n",
    "        fig.savefig(f\"colorization_{time.time()}.png\")\n",
    "        \n",
    "def log_results(loss_meter_dict):\n",
    "    for loss_name, loss_meter in loss_meter_dict.items():\n",
    "        print(f\"{loss_name}: {loss_meter.avg:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T13:18:43.256861Z",
     "iopub.status.busy": "2023-06-02T13:18:43.256525Z",
     "iopub.status.idle": "2023-06-02T13:18:43.274687Z",
     "shell.execute_reply": "2023-06-02T13:18:43.273778Z",
     "shell.execute_reply.started": "2023-06-02T13:18:43.256831Z"
    }
   },
   "outputs": [],
   "source": [
    "#Compiling the Model\n",
    "class MainModel(nn.Module):\n",
    "    def __init__(self, net_G=None, lr_G=2e-4, lr_D=2e-4, \n",
    "                 beta1=0.5, beta2=0.999, lambda_L1=100.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.lambda_L1 = lambda_L1\n",
    "        \n",
    "        if net_G is None:\n",
    "            self.net_G = init_model(Unet(input_c=1, output_c=2, n_down=8, num_filters=64), self.device)\n",
    "        else:\n",
    "            self.net_G = net_G.to(self.device)\n",
    "        self.net_D = init_model(PatchDiscriminator(input_c=3, n_down=3, num_filters=64), self.device)\n",
    "        self.GANcriterion = GANLoss(gan_mode='vanilla').to(self.device)\n",
    "        self.L1criterion = nn.L1Loss()\n",
    "        self.opt_G = optim.Adam(self.net_G.parameters(), lr=lr_G, betas=(beta1, beta2))\n",
    "        self.opt_D = optim.Adam(self.net_D.parameters(), lr=lr_D, betas=(beta1, beta2))\n",
    "    \n",
    "    def set_requires_grad(self, model, requires_grad=True):\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = requires_grad\n",
    "        \n",
    "    def setup_input(self, data):\n",
    "        self.L = data['L'].to(self.device)\n",
    "        self.ab = data['ab'].to(self.device)\n",
    "        \n",
    "    def forward(self):\n",
    "        self.fake_color = self.net_G(self.L)\n",
    "    \n",
    "    def backward_D(self):\n",
    "        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n",
    "        fake_preds = self.net_D(fake_image.detach())\n",
    "        self.loss_D_fake = self.GANcriterion(fake_preds, False)\n",
    "        real_image = torch.cat([self.L, self.ab], dim=1)\n",
    "        real_preds = self.net_D(real_image)\n",
    "        self.loss_D_real = self.GANcriterion(real_preds, True)\n",
    "        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n",
    "        self.loss_D.backward()\n",
    "    \n",
    "    def backward_G(self):\n",
    "        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n",
    "        fake_preds = self.net_D(fake_image)\n",
    "        self.loss_G_GAN = self.GANcriterion(fake_preds, True)\n",
    "        self.loss_G_L1 = self.L1criterion(self.fake_color, self.ab) * self.lambda_L1\n",
    "        self.loss_G = self.loss_G_GAN + self.loss_G_L1\n",
    "        self.loss_G.backward()\n",
    "    \n",
    "    def optimize(self):\n",
    "        self.forward()\n",
    "        self.net_D.train()\n",
    "        self.set_requires_grad(self.net_D, True)\n",
    "        self.opt_D.zero_grad()\n",
    "        self.backward_D()\n",
    "        self.opt_D.step()\n",
    "        \n",
    "        self.net_G.train()\n",
    "        self.set_requires_grad(self.net_D, False)\n",
    "        self.opt_G.zero_grad()\n",
    "        self.backward_G()\n",
    "        self.opt_G.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T13:18:43.276497Z",
     "iopub.status.busy": "2023-06-02T13:18:43.276038Z",
     "iopub.status.idle": "2023-06-02T13:18:43.290088Z",
     "shell.execute_reply": "2023-06-02T13:18:43.289067Z",
     "shell.execute_reply.started": "2023-06-02T13:18:43.276465Z"
    }
   },
   "outputs": [],
   "source": [
    "#Training Function\n",
    "def train_model(model, train_dl, epochs):\n",
    "    data = next(iter(val_dl)) # getting a batch for visualizing the model output after fixed intrvals\n",
    "    for e in range(epochs):\n",
    "        loss_meter_dict = create_loss_meters() # function returing a dictionary of objects to \n",
    "        i = 0                                  # log the losses of the complete network\n",
    "        for data in tqdm(train_dl):\n",
    "            model.setup_input(data) \n",
    "            model.optimize()\n",
    "            update_losses(model, loss_meter_dict, count=data['L'].size(0)) # function updating the log objects\n",
    "            i += 1\n",
    "            if ((e+1) % 10 == 0 or e==0) and i==500:\n",
    "                print(f\"\\nEpoch {e+1}/{epochs}\")\n",
    "                #print(f\"Iteration {i}/{len(train_dl)}\")\n",
    "                log_results(loss_meter_dict) # function to print out the losses\n",
    "                visualize(model, data, save=False) # function displaying the model's outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T13:18:43.291851Z",
     "iopub.status.busy": "2023-06-02T13:18:43.291336Z",
     "iopub.status.idle": "2023-06-02T14:16:27.715952Z",
     "shell.execute_reply": "2023-06-02T14:16:27.714889Z",
     "shell.execute_reply.started": "2023-06-02T13:18:43.291819Z"
    }
   },
   "outputs": [],
   "source": [
    "base_model = MainModel()\n",
    "train_model(base_model, train_dl, 20)\n",
    "torch.save(base_model.state_dict(), \"base-GAN.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Training the U-Net Generator with ResNet Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T14:24:32.009455Z",
     "iopub.status.busy": "2023-06-02T14:24:32.008594Z",
     "iopub.status.idle": "2023-06-02T14:24:32.018725Z",
     "shell.execute_reply": "2023-06-02T14:24:32.017446Z",
     "shell.execute_reply.started": "2023-06-02T14:24:32.009419Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,out_channels,kernel_size=3, padding=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels,kernel_size=3,padding=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.identity_map = nn.Conv2d(in_channels, out_channels,kernel_size=1,stride=stride)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, inputs):\n",
    "        x = inputs.clone().detach()\n",
    "        out = self.layer(x)\n",
    "        residual  = self.identity_map(inputs)\n",
    "        skip = out + residual\n",
    "        return self.relu(skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T14:24:37.546059Z",
     "iopub.status.busy": "2023-06-02T14:24:37.545164Z",
     "iopub.status.idle": "2023-06-02T14:24:37.552029Z",
     "shell.execute_reply": "2023-06-02T14:24:37.551004Z",
     "shell.execute_reply.started": "2023-06-02T14:24:37.546016Z"
    }
   },
   "outputs": [],
   "source": [
    "class DownSampleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.layer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T14:24:39.347633Z",
     "iopub.status.busy": "2023-06-02T14:24:39.346542Z",
     "iopub.status.idle": "2023-06-02T14:24:39.354638Z",
     "shell.execute_reply": "2023-06-02T14:24:39.353591Z",
     "shell.execute_reply.started": "2023-06-02T14:24:39.347592Z"
    }
   },
   "outputs": [],
   "source": [
    "class UpSampleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.res_block = ResBlock(in_channels + out_channels, out_channels)\n",
    "        \n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.upsample(inputs)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.res_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T14:24:46.331818Z",
     "iopub.status.busy": "2023-06-02T14:24:46.331425Z",
     "iopub.status.idle": "2023-06-02T14:24:46.342403Z",
     "shell.execute_reply": "2023-06-02T14:24:46.341416Z",
     "shell.execute_reply.started": "2023-06-02T14:24:46.331787Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_channel, output_channel, dropout_rate = 0.2):\n",
    "        super().__init__()\n",
    "        self.encoding_layer1_ = ResBlock(input_channel,64)\n",
    "        self.encoding_layer2_ = DownSampleConv(64, 128)\n",
    "        self.encoding_layer3_ = DownSampleConv(128, 256)\n",
    "        self.bridge = DownSampleConv(256, 512)\n",
    "        self.decoding_layer3_ = UpSampleConv(512, 256)\n",
    "        self.decoding_layer2_ = UpSampleConv(256, 128)\n",
    "        self.decoding_layer1_ = UpSampleConv(128, 64)\n",
    "        self.output = nn.Conv2d(64, output_channel, kernel_size=1)\n",
    "        self.dropout = nn.Dropout2d(dropout_rate)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        ###################### Enocoder #########################\n",
    "        e1 = self.encoding_layer1_(inputs)\n",
    "        e1 = self.dropout(e1)\n",
    "        e2 = self.encoding_layer2_(e1)\n",
    "        e2 = self.dropout(e2)\n",
    "        e3 = self.encoding_layer3_(e2)\n",
    "        e3 = self.dropout(e3)\n",
    "        \n",
    "        ###################### Bridge #########################\n",
    "        bridge = self.bridge(e3)\n",
    "        bridge = self.dropout(bridge)\n",
    "        \n",
    "        ###################### Decoder #########################\n",
    "        d3 = self.decoding_layer3_(bridge, e3)\n",
    "        d2 = self.decoding_layer2_(d3, e2)\n",
    "        d1 = self.decoding_layer1_(d2, e1)\n",
    "        \n",
    "        ###################### Output #########################\n",
    "        output = self.output(d1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T14:24:52.165702Z",
     "iopub.status.busy": "2023-06-02T14:24:52.164688Z",
     "iopub.status.idle": "2023-06-02T14:24:52.173220Z",
     "shell.execute_reply": "2023-06-02T14:24:52.172027Z",
     "shell.execute_reply.started": "2023-06-02T14:24:52.165646Z"
    }
   },
   "outputs": [],
   "source": [
    "def pretrain_generator(net_G, train_dl, opt, criterion, epochs):\n",
    "    for e in range(epochs):\n",
    "        loss_meter = AverageMeter()\n",
    "        for data in tqdm(train_dl):\n",
    "            L, ab = data['L'].to(device), data['ab'].to(device)\n",
    "            preds = net_G(L)\n",
    "            loss = criterion(preds, ab)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            loss_meter.update(loss.item(), L.size(0))\n",
    "            \n",
    "        print(f\"Epoch {e + 1}/{epochs}\")\n",
    "        print(f\"L1 Loss: {loss_meter.avg:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T14:27:47.388579Z",
     "iopub.status.busy": "2023-06-02T14:27:47.387768Z",
     "iopub.status.idle": "2023-06-02T15:42:09.675941Z",
     "shell.execute_reply": "2023-06-02T15:42:09.674582Z",
     "shell.execute_reply.started": "2023-06-02T14:27:47.388547Z"
    }
   },
   "outputs": [],
   "source": [
    "net_G = Generator(1,2).to(device)\n",
    "opt = optim.Adam(net_G.parameters(), lr=1e-4)\n",
    "criterion = nn.L1Loss()        \n",
    "pretrain_generator(net_G, train_dl, opt, criterion, 20)\n",
    "torch.save(net_G.state_dict(), \"res18-unet.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T15:42:09.679169Z",
     "iopub.status.busy": "2023-06-02T15:42:09.678747Z",
     "iopub.status.idle": "2023-06-02T17:24:00.332986Z",
     "shell.execute_reply": "2023-06-02T17:24:00.331901Z",
     "shell.execute_reply.started": "2023-06-02T15:42:09.679126Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net_G = Generator(1, 2).to(device)\n",
    "net_G.load_state_dict(torch.load(\"res18-unet.pt\", map_location=device))\n",
    "model = MainModel(net_G=net_G)\n",
    "train_model(model, train_dl, 20)\n",
    "torch.save(model.state_dict(), \"final_model_weights.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
